{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Azure OpenAI Batch Jobs ##\n",
    "Requirements prior to running this code a deployment resource in Azure OpenAI with the deployment type 'Global-Batch' enabled\n",
    "Initialize the initial library packages that are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs openai\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the files we require to call for a batch file ##\n",
    "This is very familiar with the format that is used for fine-tuning .jsonl(json lines) here is the example we can use for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"REPLACE-WITH-MODEL-DEPLOYMENT-NAME\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What’s in this image?\"},{\"type\": \"image_url\",\"image_url\": {\"url\": \"https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/main/articles/ai-services/openai/media/how-to/generated-seattle.png\"}}]}],\"max_tokens\": 1000}}\n",
    "{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"REPLACE-WITH-MODEL-DEPLOYMENT-NAME\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What’s in this image?\"},{\"type\": \"image_url\",\"image_url\": {\"url\": \"https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/main/articles/ai-services/openai/media/how-to/generated-seattle.png\"}}]}],\"max_tokens\": 1000}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the batch file ##\n",
    "You want to use the images.jsonl as a exmaple if you are trying to follow along but for the purposes of the blog post I'm using the images.jsonl for image capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "    \n",
    "# load the env variables (this is include in .gitignore)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Initialize the AzureOpenAI client    \n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-07-01-preview\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    )\n",
    "\n",
    "# Upload a file with a purpose of \"batch\" change to the purpose you want in current dir i'm using images.jsonl\n",
    "file = client.files.create(\n",
    "  file=open(\"images.jsonl\", \"rb\"), \n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n",
    "print(file.model_dump_json(indent=2))\n",
    "file_id = file.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking the actual job from the submission to the batch job ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the uploaded file is in processed state\n",
    "import time\n",
    "import datetime \n",
    "\n",
    "status = \"pending\"\n",
    "while status != \"processed\":\n",
    "    time.sleep(15)\n",
    "    file_response = client.files.retrieve(file_id)\n",
    "    status = file_response.status\n",
    "    print(f\"{datetime.datetime.now()} File Id: {file_id}, Status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our batch job ##\n",
    "So we now have a file that is processed and ready to submit to our API for processing this code will use the client.batches.create() for this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit a batch job with the file\n",
    "batch_response = client.batches.create(\n",
    "    input_file_id=file_id,\n",
    "    endpoint=\"/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    ")\n",
    "\n",
    "# Save batch ID for later use\n",
    "batch_id = batch_response.id\n",
    "\n",
    "print(batch_response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking the batch job in progress ##\n",
    "So we've submitted our job but what is the process look like? This can be caught with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime \n",
    "\n",
    "# What status is current? We saw that this is validating once sent, the while statement is looking for three conditions with a timer.\n",
    "status = \"validating\"\n",
    "while status not in (\"completed\", \"failed\", \"canceled\"):\n",
    "    time.sleep(60)\n",
    "    batch_response = client.batches.retrieve(batch_id)\n",
    "    status = batch_response.status\n",
    "    print(f\"{datetime.datetime.now()} Batch Id: {batch_id},  Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the batch job output file ##\n",
    "If you need to debug you'll have to reference the error_file_id and a separate output_file_id for retrieval we will run the following json for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Takes the file response id\n",
    "file_response = client.files.content(batch_response.output_file_id)\n",
    "raw_responses = file_response.text.strip().split('\\n')  \n",
    "# Runs a for loop across the raw responses and puts out the json response and formatted json\n",
    "for raw_response in raw_responses:  \n",
    "    json_response = json.loads(raw_response)  \n",
    "    formatted_json = json.dumps(json_response, indent=2)  \n",
    "    print(formatted_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
